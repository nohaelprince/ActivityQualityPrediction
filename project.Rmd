---
title: "project"
author: "Noha Elprince"
date: "November 23, 2014"
output: html_document
---
## Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

exactly according to the specification (Class A)
throwing the elbows to the front (Class B)
lifting the dumbbell only halfway (Class C)
lowering the dumbbell only halfway (Class D)
throwing the hips to the front (Class E)

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

The data for this project come from this source: <http://groupware.les.inf.puc-rio.br/har>

The main objectives of this project are as follows

* Predict the manner in which they did the exercise
* Build a prediction model
* Calculate the out of sample error.
* Use the prediction model to predict 20 different test cases provided


## Load Data
```{r, echo=TRUE}
  training <- read.csv(file="./data/pml-training.csv", head=TRUE, na.strings=c("NA",""))
  testing <- read.csv(file="data/pml-testing.csv", head=TRUE,na.strings=c("NA",""))
  dim(training)   #[1] 19622   160
  dim(testing)    #[1]  20 160
  # str(training)
```

The dataset comprises `r ncol(testing)` features and `r  nrow(training)` observations in the training set and  `r  nrow(testing)` test cases in the testing set.

## Processing data
First, we check how many columns have NA values in the training and testing data and what is the quantity of NA values present.

```{r, echo=TRUE}
  sum(is.na(training)) #[1] 1921600
  sum(is.na(testing))  #[1] 2000 
```


we are going to ignore NA values using the following code segment

```{r, echo=TRUE}
# for training dataset
columnNACounts <- colSums(is.na(training)) 
# columnNACounts 
# after checking columnNACounts , we noticed:
# most columns with NA values have sum of NA values exceeeds 19200 
badColumns <- columnNACounts >= 19200           
cleanTrainingdata <- training[!badColumns]        
sum(is.na(cleanTrainingdata)) # 0

# same for testing dataset
columnNACounts <- colSums(is.na(testing))  
# columnNACounts 
# after checking columnNACounts , we noticed:
# most columns with NA values have sum of NA values exceeeds 20
badColumns <- columnNACounts >= 20                
cleanTestingdata <- testing[!badColumns]       
sum(is.na(cleanTestingdata)) # 0                   
```

## Feature Selection

 
### Exploratory Data Analysis
```{r, echo=TRUE}
  plot(cleanTrainingdata$classe,col=rainbow(5),main = "classe frequency plot")
```

Now we start partitioning the data:

### Partition cleaned training data : A training set and a cross validation set.
```{r, echo=TRUE}
inTrain <- createDataPartition(y = cleanTrainingdata$classe, p = 0.6, list = FALSE)
training <- cleanTrainingdata[inTrain, ]
crossval <- cleanTrainingdata[-inTrain, ]
```

### Fit a random forest predictor relating the factor variable classe to the remaining variables.
```{r, echo=TRUE}
model <- randomForest(classe ~ ., data = cleanTrainingdata, importance = FALSE)

vimp <- varImp(model, scale=FALSE)
vimp <- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE] # order descendingly
vimp$var <- rownames(vimp)    # add rownames              
# vimp # print all the important features ordered based on the most important.
# plot the 5 top important features
barplot(vimp$Overall[1:10], main="Top 5 Important Features", horiz=TRUE, col=rainbow(10), 
  names.arg=vimp$var[1:10])
```

### Build the model using 5-fold cross validation

Here, we calculate the in sample accuracy which is the prediction accuracy of our model on the training data set.

```{r, echo=TRUE}
predictCrossVal <- predict(model, cleanTrainingData)# We build the model using 5-fold cross validation.
confusionMatrix(crossval$classe, predictCrossVal)
```
Thus, from the above confusion matrix, sample accuracy value is 100%.

### Calculate out-of-sample accuracy
```{r, echo=TRUE}
testing_pred <- predict(model, crossval)
confusionMatrix(testing_pred, crossval$classe)

```

Now, we apply the above model to the clean testing data (20 cases)

```{r, echo=TRUE}
answers <- predict(model, cleanTestingdata)
answers <- as.character(answers)
answers
```





### Apply the same treatment to the final testing data
```{r, echo=TRUE}
# remove features with near zero variability
nsv <- nearZeroVar(testing,saveMetrics=TRUE) 
dim(testing) # [1]  20 160
removed_features <- testing[, nzv] # check removed features
names(removed_features)
filteredDescr <- testing[, -nzv]
dim(filteredDescr) # [1]  20 100
# remove columns with NA
data_test_NAs <- apply(filteredDescr, 2, function(x) {sum(is.na(x))})
data_test_clean <- data_test[,which(data_test_NAs == 0)]
# remove the first 7 columns as they include names and timestamps
data_test_clean <- data_test_clean[8:length(data_test_clean)]
dim(data_test_clean) # [1] 20 52
```
The cleaned testing data has same no. of features (52) as the cleaned training data.
Now, we are ready for prediction...

### predict the classes of the test set
```{r, echo=TRUE}
predictTest <- predict(model, data_test_clean)
```


set.seed(33833)

# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(data$classe ~ ., data = training, importance = FALSE)
b <- varImp(a)
order(b)

```